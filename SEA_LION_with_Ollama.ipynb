{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook enables anyone with a Google Colab account to experiment with AI Singapore's SEA-LION models with Ollama. At the time of writing, Google Colab provides a free T4 GPU (16GB GDDR6) runtime.\n",
        "\n",
        "Please visit https://sea-lion.ai/ for more information about the models."
      ],
      "metadata": {
        "id": "RDdKgYDoj67l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOdN9OAgIjTs",
        "outputId": "335318f3-2c2a-4cb8-e6c8-92f0657efc99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lshw is installed successfully.\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Install the dependency\n",
        "!sudo apt install lshw > apt.log 2>&1 && echo \"lshw is installed successfully.\"\n",
        "\n",
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the Ollama service\n",
        "!ollama serve > ollama.log 2>&1 &"
      ],
      "metadata": {
        "id": "0SRw3kNYJPWb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pull either one or both of the SEA-LION models."
      ],
      "metadata": {
        "id": "A5Ic7veAjftY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the Gemma 2 9B CPT SEA-Lionv3 Instruct model\n",
        "!ollama pull aisingapore/gemma2-9b-cpt-sea-lionv3-instruct:q4_k_m > ollama_pull.log 2>&1 && echo \"The model is pulled successfully.\""
      ],
      "metadata": {
        "id": "sC1UTmFd5ZRj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abb96db-8a74-44db-a942-be52051a948a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model is pulled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull the Llama 3.1 8B CPT SEA-Lionv3 Instruct model\n",
        "!ollama pull aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct:q4_k_m > ollama_pull.log 2>&1 && echo \"The model is pulled successfully.\""
      ],
      "metadata": {
        "id": "FhgpVb57jQc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602d41ae-d194-47e8-8d44-fd89ff872a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model is pulled successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbkhdD1aYM5S",
        "outputId": "83bab9d2-d423-48f3-a1ee-894fc2a83260"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME                                                    ID              SIZE      MODIFIED      \n",
            "aisingapore/gemma2-9b-cpt-sea-lionv3-instruct:q4_k_m    600c92976f57    5.8 GB    3 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The simple chat demo is modified from https://github.com/ollama/ollama/blob/main/examples/python-simplechat/client.py\n",
        "# Please modify it for your use cases.\n",
        "\n",
        "import json\n",
        "import requests\n",
        "\n",
        "# If Ollama is not running, start `ollama serve` in a previous cell\n",
        "# Use the Gemma 2 9B CPT SEA-Lionv3 Instruct model\n",
        "model = \"aisingapore/gemma2-9b-cpt-sea-lionv3-instruct:q4_k_m\"\n",
        "\n",
        "# Uncomment this statement to use the Llama 3.1 8B CPT SEA-Lionv3 Instruct model\n",
        "#model = \"aisingapore/llama3.1-8b-cpt-sea-lionv3-instruct:q4_k_m\"\n",
        "\n",
        "def chat(messages):\n",
        "    r = requests.post(\n",
        "        \"http://0.0.0.0:11434/api/chat\",\n",
        "        json={\"model\": model, \"messages\": messages, \"stream\": True},\n",
        "        stream=True\n",
        "    )\n",
        "    r.raise_for_status()\n",
        "    output = \"\"\n",
        "\n",
        "    for line in r.iter_lines():\n",
        "        body = json.loads(line)\n",
        "        if \"error\" in body:\n",
        "            raise Exception(body[\"error\"])\n",
        "        if body.get(\"done\") is False:\n",
        "            message = body.get(\"message\", \"\")\n",
        "            content = message.get(\"content\", \"\")\n",
        "            output += content\n",
        "            print(content, end=\"\", flush=True)\n",
        "\n",
        "        if body.get(\"done\", False):\n",
        "            message[\"content\"] = output\n",
        "            return message\n",
        "\n",
        "\n",
        "def main():\n",
        "    messages = []\n",
        "    print (\"Input /bye or nothing to exit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter a prompt: \").strip()\n",
        "        if not user_input or len(user_input) == 0 or user_input == '/bye':\n",
        "            print(\"Exited.\")\n",
        "            break\n",
        "\n",
        "        print()\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "        message = chat(messages)\n",
        "        messages.append(message)\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "9Is0yORkV9AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34606b6f-7403-4378-b55d-ec632b8733a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input /bye or nothing to exit.\n",
            "Enter a prompt: Translate \\\"an apple a day keeps the doctor away\\\" to SEA languages.\n",
            "\n",
            "Here are translations of \"An apple a day keeps the doctor away\" into several Southeast Asian languages:\n",
            "\n",
            "* **Indonesian:** Sebutir apel sehari jauhkan dokter pergi.\n",
            "* **Malaysian (Bahasa Melayu):** Seekor apel setiap hari menjauhkan doktor.\n",
            "* **Thai:** แอปเปิลวันละลูก หมอไม่ต้องมา (Apel wan la luk, mao mai dtao ma)\n",
            "* **Vietnamese:** Một quả táo mỗi ngày giữ bác sĩ xa (Môt quả táo mỗi ngày giữ bác sĩ xa)\n",
            "* **Filipino:** Isang mansanas araw-araw ay nagpapalayo sa doktor.\n",
            "* **Myanmar (Burmese):** နေ့စဥ် လတ်သော အပပဲလ တစ်ခု ရသည်မှာ ဆရာဝန်ကို မကူး။ (Nètsù lattasau appeleitkʰo yathāmiṁa sărāwun ko maka)\n",
            "\n",
            "**Note:** These translations aim for accuracy while preserving the idiomatic meaning of the proverb.  Slight variations might exist depending on regional dialects. \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Enter a prompt: \n",
            "Exited.\n"
          ]
        }
      ]
    }
  ]
}